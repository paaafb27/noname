"""
QUASARZONE í¬ë¡¤ëŸ¬

URL: https://quasarzone.com/bbs/qb_saleinfo
"""
import datetime
import time

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import sys
import os
import re

from webdriver_manager.core.os_manager import ChromeType

from common.log_util import log_item
from common.number_extractor import extract_shipping_fee
from common.store_extractor import extract_store
from common.filter_by_regtime import filter_by_time, parse_time, to_iso8601

# common ëª¨ë“ˆ
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'common'))


class QuasarzoneScraper:

    def __init__(self):
        self.url = 'https://quasarzone.com/bbs/qb_saleinfo'
        self.main_url = 'https://quasarzone.com'
        self.source_site = 'QUASARZONE'
        self.max_pages = 3  # ìµœëŒ€ í˜ì´ì§€ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        self.test_mode = False

        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„í„°ë§ ì‹œê°„ ì½ê¸° (ê¸°ë³¸ê°’ 30ë¶„)
        self.filter_minutes = int(os.environ.get('FILTER_MINUTES', 30))

    def scrape(self):
        """í˜ì´ì§• í¬ë¡¤ë§ (30ë¶„ í•„í„°ë§)"""
        return self._scrape_with_pagination()

    def _scrape_with_pagination(self):
        """
        - í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ ê²Œì‹œê¸€ì´ 30ë¶„ ì´ë‚´ë©´ ë‹¤ìŒ í˜ì´ì§€ ê³„ì† í™•ì¸
        - ë§ˆì§€ë§‰ ê²Œì‹œê¸€ì´ 30ë¶„ ì´ˆê³¼í•˜ê±°ë‚˜ ìµœëŒ€ í˜ì´ì§€ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
        """

        all_items = []
        page_num = 1
        driver = None

        kst = datetime.timezone(datetime.timedelta(hours=9))
        filter_minutes = self.filter_minutes
        now = datetime.datetime.now(kst)
        cutoff_time = now - datetime.timedelta(minutes=filter_minutes)

        try:
            driver = self._create_driver()
            print(f"Chrome ë¸Œë¼ìš°ì € ì‹œì‘ : {self.url}")

            while page_num <= self.max_pages:
                print(f"\n{page_num}í˜ì´ì§€ í¬ë¡¤ë§...")

                # âœ… ê°™ì€ driver ì¬ì‚¬ìš©
                page_items = self._scrape_page(driver, page_num)

                if not page_items:
                    print(f"{page_num}í˜ì´ì§€: ê²Œì‹œê¸€ ì—†ìŒ, ì¢…ë£Œ")
                    break

                # 30ë¶„ ì´ë‚´ ì‘ì„±ëœ ê²Œì‹œê¸€ í•„í„°ë§
                page_filtered = filter_by_time(page_items, minutes=filter_minutes)
                if page_filtered:
                    print(f"ìˆ˜ì§‘ ëŒ€ìƒ {len(page_filtered)}ê°œ:")
                    for filtered_item in page_filtered:
                        log_item(filtered_item)

                all_items.extend(page_filtered)
                print(f"{page_num}í˜ì´ì§€: {len(page_items)}ê°œ â†’ í•„í„°ë§ {len(page_filtered)}ê°œ")

                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ì—¬ë¶€ íŒë‹¨
                last_item = page_items[-1]
                last_time = parse_time(last_item.get('crawledAt', ''))

                if not last_time:
                    print(f"ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨, í¬ë¡¤ë§ ì¢…ë£Œ")
                    break

                if last_time < cutoff_time:
                    print(f"ë§ˆì§€ë§‰ ê²Œì‹œê¸€ {filter_minutes}ë¶„ ì´ˆê³¼ ({last_time.strftime('%H:%M:%S')}), ì¢…ë£Œ")
                    break

                print(f"ë§ˆì§€ë§‰ ê²Œì‹œê¸€ {filter_minutes}ë¶„ ì´ë‚´ ({last_time.strftime('%H:%M:%S')}), ë‹¤ìŒ í˜ì´ì§€ í™•ì¸")

                # í˜ì´ì§€ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                driver.execute_script("window.stop();")
                driver.delete_all_cookies()

                page_num += 1

            if page_num > self.max_pages:
                print(f"\nìµœëŒ€ í˜ì´ì§€({self.max_pages}) ë„ë‹¬, í¬ë¡¤ë§ ì¢…ë£Œ")

            print(f"\nì´ {len(all_items)}ê°œ ìˆ˜ì§‘\n")
            return all_items

        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return all_items            # ìˆ˜ì§‘ëœ ë°ì´í„°ë¼ë„ ë°˜í™˜

        finally:
            if driver:
                try:
                    driver.quit()
                    print("Chrome ë¸Œë¼ìš°ì € ì •ìƒ ì¢…ë£Œ")
                except Exception as e:
                    print(f"Chrome ì¢…ë£Œ ì¤‘ ì—ëŸ¬: {e}")

    def _create_driver(self):
        options = Options()

        # --- Fargate/Lambda ê³µí†µ ì˜µì…˜ (ìµœì†Œ ì˜µì…˜ ìœ ì§€) ---
        print("(ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ì‹¤í–‰ - WebDriverManager ì‚¬ìš©)")

        options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

        options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--referer=https://www.google.com/')

        # ìë™í™” ê°ì§€ ìš°íšŒ
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        # options.add_argument('--proxy-server=socks5://proxy-server:1080')

        # [ìˆ˜ì •] ì„ì‹œ ë””ë ‰í† ë¦¬ ì˜µì…˜ì€ ì¶©ëŒ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ ì¼ë‹¨ ì œê±°í•˜ê³  í…ŒìŠ¤íŠ¸
        # options.add_argument('--user-data-dir=/tmp/chrome-user-data')
        # options.add_argument('--disk-cache-dir=/tmp/chrome-cache-dir')
        # options.add_argument('--data-path=/tmp/chrome-data-path')

        try:
            print("WebDriverManagerë¡œ Chromedriver ê²½ë¡œ í™•ì¸ ë° ë“œë¼ì´ë²„ ìƒì„± ì‹œë„...")
            # ğŸ’¡ [í•„ìˆ˜ ìˆ˜ì •] WebDriverManager ì‚¬ìš©
            #   Service ê°ì²´ì— ìë™ìœ¼ë¡œ ë“œë¼ì´ë²„ ê²½ë¡œë¥¼ ì°¾ì•„ ì „ë‹¬
            service = Service('/usr/local/bin/chromedriver')
            # service = Service('/usr/local/bin/chromedriver').install())
            driver = webdriver.Chrome(service=service, options=options)
            print("Chrome ë“œë¼ì´ë²„ ìƒì„± ì„±ê³µ!")

            # WebDriver ì†ì„± ìˆ¨ê¸°ê¸°
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            driver.set_page_load_timeout(60)
            return driver
        except Exception as e:
            print(f"!!!!!!!! Chrome ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨ !!!!!!!!!!")
            print(f"ì˜¤ë¥˜: {e}")
            # WebDriverManager ë¡œê·¸ í™•ì¸ì„ ìœ„í•´ traceback ì¶”ê°€
            import traceback
            traceback.print_exc()
            raise # ì—ëŸ¬ ë‹¤ì‹œ ë°œìƒ
        else:
            print("  (ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰)")
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument(f'--user-agent={user_agent_string}')
            options.add_argument('--window-size=1920,1080')
            driver = webdriver.Chrome(options=options)

        # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ê³µí†µ)
        driver.set_page_load_timeout(60)
        return driver

    def _scrape_page(self, driver, page_num):

        items = []
        html = None

        try:
            # í˜ì´ì§€ URL
            if page_num == 1:
                url = self.url
            else:
                url = f"{self.url}&page={page_num}"

            driver.get(url)

            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div.market-type-list"))
                )
                print("ê²Œì‹œê¸€ ë¡œë“œ í™•ì¸")
            except Exception as e:
                # íƒ€ì„ì•„ì›ƒ ë˜ì–´ë„ ê³„ì† ì§„í–‰ (ë¶€ë¶„ ë°ì´í„°ë¼ë„ ìˆ˜ì§‘)
                print(f"ëª…ì‹œì  ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ (ê³„ì† ì§„í–‰): {e}")


            # HTML íŒŒì‹±
            html = driver.page_source
            soup = BeautifulSoup(html, 'lxml')

            # ê²Œì‹œê¸€ ëª©ë¡
            rows = soup.select('div.market-type-list tbody tr')
            print(f"ê²Œì‹œê¸€ {len(rows)}ê°œ ë°œê²¬")

            # ë‹¤ë¥¸ ì„ íƒìë“¤ë„ ì‹œë„
            if len(rows) == 0:
                print(f"  [DEBUG] ë‹¤ë¥¸ ì„ íƒì ì‹œë„...")
                alternative_rows = soup.select('div.market-type-list')
                print(f"  [DEBUG] ëŒ€ì²´ ì„ íƒì: {len(alternative_rows)}ê°œ")

            for row in rows:
                try:
                    # ë°ì´í„° ì¶”ì¶œ
                    item = self._extract_item(row, driver)

                    if item:
                        items.append(item)

                except Exception as e:
                    print(f"ê²Œì‹œê¸€ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

        except Exception as e:
            print(f"  í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

        finally:
            html = None
            soup = None

        return items

    def _extract_item(self, row, driver):
        """
        ì„¸ì¼ì •ë³´ ì¶”ì¶œ
        """
        import re

        # ì œëª©
        title_element = row.select_one('tr span.ellipsis-with-reply-cnt')
        title = title_element.get_text(strip=True) if title_element else None

        # ê°€ê²© : ï¿¦ 19,800 (KRW)
        price = None
        price_element = row.select_one('span.text-orange')
        if price_element:
            price_text = price_element.get_text(strip=True)
            price_match = re.search(r'[ï¿¦â‚©]\s*([0-9,]+)', price_text)
            if price_match:
                try:
                    price = int(price_match.group(1).replace(',', ''))
                except:
                    pass

        # ë°°ì†¡ë¹„
        shipping_fee_element = row.select_one('div.market-info-sub > p:first-of-type > span:last-of-type')
        shipping_fee = shipping_fee_element.get_text(strip=True) if price_element else None
        shipping_fee = extract_shipping_fee(shipping_fee)

        # íŒë§¤ì²˜
        store = extract_store(title, self.source_site)
        """
        if store:
            store_match = re.match(r'\[([^\]]+)\]', title)
            if store_match:
                store = store_match.group(1)
                title = title[store_match.end():].strip()
        """

        # ëŒ“ê¸€ ìˆ˜
        reply_element = row.select_one('span.board-list-comment span.ctn-count')
        reply_count = reply_element.get_text(strip=True) if reply_element else 0

        # ì¢‹ì•„ìš” ìˆ˜
        like_element = row.select_one('span.num.num.tp2')
        like_count = like_element.get_text(strip=True) if like_element else 0

        # ì¹´í…Œê³ ë¦¬
        category_element = row.select_one('span.category')
        category = category_element.get_text(strip=True) if category_element else None

        # URL
        url_element = row.select_one('p.tit a')
        product_url = self.main_url + url_element['href']

        # ì´ë¯¸ì§€ url
        image_element = row.select_one('a.thumb img')
        image_url = image_element['src'] if image_element else None

        # ë“±ë¡ ì‹œê°„
        time_element = row.select_one('span.date')
        time_text = time_element.get_text(strip=True) if time_element else None
        if time_text and 'ë¶„' in time_text:
            # ì‹œê°„ ì¶”ì¶œ
            import re
            match = re.search(r'(\d+)(ë¶„|ì‹œê°„)', time_text)

            if match:
                value = int(match.group(1))
                if value <= 30:
                    # ìƒì„¸ í˜ì´ì§€ ì ‘ì† í›„ ì‹œê°„ get
                    try:
                        current_url = driver.current_url  # í˜„ì¬ URL ì €ì¥
                        driver.get(product_url)

                        time_element_selector = 'div.util-area span.date'
                        wait = WebDriverWait(driver, 10)
                        time_element = wait.until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, time_element_selector)))

                        time_text = time_element.text
                        time = to_iso8601(parse_time(time_text))

                        # ì›ë˜ í˜ì´ì§€ë¡œ ë³µê·€
                        driver.get(current_url)

                    except Exception as e:
                        print(f"  [ìƒì„¸ ì‹¤íŒ¨] {e}, ëª©ë¡ ì‹œê°„ ì‚¬ìš©")
                        time = to_iso8601(parse_time(time_text))

                else:
                    # 30ë¶„ ì´ˆê³¼
                    time = to_iso8601(parse_time(time_text))
            else:
                # ì •ê·œì‹ ë§¤ì¹­ ì‹¤íŒ¨ > ë‚ ì§œ í˜•ì‹
                time = to_iso8601(parse_time(time_text))

        else:
            time = None

        return {
            'title': title,
            'price': price,
            'storeName': store,
            'category': category,
            'shippingFee': shipping_fee,
            'productUrl': product_url,
            'imageUrl': image_url,
            'replyCount': reply_count,
            'likeCount': like_count,
            'sourceSite': self.source_site,
            'crawledAt': time
        }


