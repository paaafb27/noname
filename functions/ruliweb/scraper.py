"""
ë£¨ë¦¬ì›¹ í¬ë¡¤ëŸ¬

URL: https://bbs.ruliweb.com/market/board/1020
í•„í„°ë§: notice, company, best í´ë˜ìŠ¤ ì œì™¸
íŒë§¤ì²˜: <span class="subject_tag"> ë˜ëŠ” ì œëª©ì—ì„œ ì¶”ì¶œ
"""
import datetime
import random
import time

from selenium import webdriver
from selenium.webdriver import ActionChains
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException,
    NoSuchElementException,
    ElementClickInterceptedException,
    ElementNotInteractableException,
)
from common.number_extractor import (
    extract_price_from_text,
    extract_shipping_fee,
    clean_title,
    extract_comment_count_from_title,
    format_price, extract_number_from_text
)
from bs4 import BeautifulSoup
import sys
import os
import re
import boto3  # [ì¶”ê°€] S3 ì—…ë¡œë“œë¥¼ ìœ„í•´ import

from webdriver_manager.core.os_manager import ChromeType

from common.log_util import log_item
from common.store_extractor import clean_store_name

# common ëª¨ë“ˆ
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from common.filter_by_regtime import filter_by_time, parse_time, to_iso8601



class RuliwebScraper:

    def __init__(self):
        self.url = 'https://bbs.ruliweb.com/market/board/1020'
        self.main_url = 'https://www.ruliweb.com/'
        self.source_site = 'RULIWEB'
        self.max_pages = 3
        self.test_mode = False

        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„í„°ë§ ì‹œê°„ ì½ê¸° (ê¸°ë³¸ê°’ 30ë¶„)
        self.filter_minutes = int(os.environ.get('FILTER_MINUTES', 30))

        # [ì¶”ê°€] ë””ë²„ê¹… íŒŒì¼ì„ ì €ì¥í•  S3 ë²„í‚· ì´ë¦„ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        self.s3_bucket_name = os.environ.get('S3_BUCKET_NAME')

    def scrape(self):
        """í˜ì´ì§• í¬ë¡¤ë§ (30ë¶„ í•„í„°ë§)"""
        return self._scrape_with_pagination()

    def _scrape_with_pagination(self):
        """
        - í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ ê²Œì‹œê¸€ì´ 30ë¶„ ì´ë‚´ë©´ ë‹¤ìŒ í˜ì´ì§€ ê³„ì† í™•ì¸
        - ë§ˆì§€ë§‰ ê²Œì‹œê¸€ì´ 30ë¶„ ì´ˆê³¼í•˜ê±°ë‚˜ ìµœëŒ€ í˜ì´ì§€ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
        """

        all_items = []
        page_num = 1
        driver = None

        kst = datetime.timezone(datetime.timedelta(hours=9))
        filter_minutes = self.filter_minutes
        now = datetime.datetime.now(kst)
        cutoff_time = now - datetime.timedelta(minutes=filter_minutes)

        try:
            driver = self._create_driver()
            print("Chrome ë¸Œë¼ìš°ì € ì‹œì‘")

            while page_num <= self.max_pages:
                print(f"\n{page_num}í˜ì´ì§€ í¬ë¡¤ë§...")

                # ê°™ì€ driver ì¬ì‚¬ìš©
                page_items = self._scrape_page(driver, page_num)

                if not page_items:
                    print(f"{page_num}í˜ì´ì§€: ê²Œì‹œê¸€ ì—†ìŒ, ì¢…ë£Œ")
                    break

                # 30ë¶„ ì´ë‚´ ì‘ì„±ëœ ê²Œì‹œê¸€ í•„í„°ë§
                page_filtered = filter_by_time(page_items, minutes=filter_minutes)
                if page_filtered:
                    print(f"ìˆ˜ì§‘ ëŒ€ìƒ {len(page_filtered)}ê°œ:")
                    for filtered_item in page_filtered:
                        log_item(filtered_item)

                all_items.extend(page_filtered)
                print(f"{page_num}í˜ì´ì§€: {len(page_items)}ê°œ â†’ í•„í„°ë§ {len(page_filtered)}ê°œ")

                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ì—¬ë¶€ íŒë‹¨
                last_item = page_items[-1]
                last_time = parse_time(last_item.get('crawledAt', ''))

                if not last_time:
                    print(f"ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨, í¬ë¡¤ë§ ì¢…ë£Œ")
                    break

                if last_time < cutoff_time:
                    print(f"ë§ˆì§€ë§‰ ê²Œì‹œê¸€ {filter_minutes}ë¶„ ì´ˆê³¼ ({last_time.strftime('%H:%M:%S')}), ì¢…ë£Œ")
                    break

                print(f"ë§ˆì§€ë§‰ ê²Œì‹œê¸€ {filter_minutes}ë¶„ ì´ë‚´ ({last_time.strftime('%H:%M:%S')}), ë‹¤ìŒ í˜ì´ì§€ í™•ì¸")

                # í˜ì´ì§€ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                driver.execute_script("window.stop();")
                driver.delete_all_cookies()

                page_num += 1

            if page_num > self.max_pages:
                print(f"\nìµœëŒ€ í˜ì´ì§€({self.max_pages}) ë„ë‹¬, í¬ë¡¤ë§ ì¢…ë£Œ")

            print(f"\nì´ {len(all_items)}ê°œ ìˆ˜ì§‘\n")
            return all_items

        except Exception as e:
            print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return all_items  # ìˆ˜ì§‘ëœ ë°ì´í„°ë¼ë„ ë°˜í™˜

        finally:
            if driver:
                try:
                    driver.quit()
                    print("Chrome ë¸Œë¼ìš°ì € ì •ìƒ ì¢…ë£Œ")
                except Exception as e:
                    print(f"Chrome ì¢…ë£Œ ì¤‘ ì—ëŸ¬: {e}")

    def _create_driver(self):
        options = Options()

        # --- Fargate/Lambda ê³µí†µ ì˜µì…˜ (ìµœì†Œ ì˜µì…˜ ìœ ì§€) ---
        print("(ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ì‹¤í–‰ - WebDriverManager ì‚¬ìš©)")

        options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

        options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--referer=https://www.google.com/')

        # ìë™í™” ê°ì§€ ìš°íšŒ
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)

        # [ìˆ˜ì •] ì„ì‹œ ë””ë ‰í† ë¦¬ ì˜µì…˜ì€ ì¶©ëŒ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ ì¼ë‹¨ ì œê±°í•˜ê³  í…ŒìŠ¤íŠ¸
        # options.add_argument('--user-data-dir=/tmp/chrome-user-data')
        # options.add_argument('--disk-cache-dir=/tmp/chrome-cache-dir')
        # options.add_argument('--data-path=/tmp/chrome-data-path')

        try:
            print("WebDriverManagerë¡œ Chromedriver ê²½ë¡œ í™•ì¸ ë° ë“œë¼ì´ë²„ ìƒì„± ì‹œë„...")
            # ğŸ’¡ [í•„ìˆ˜ ìˆ˜ì •] WebDriverManager ì‚¬ìš©
            #   Service ê°ì²´ì— ìë™ìœ¼ë¡œ ë“œë¼ì´ë²„ ê²½ë¡œë¥¼ ì°¾ì•„ ì „ë‹¬
            service = Service('/usr/local/bin/chromedriver')
            # service = Service('/usr/local/bin/chromedriver').install())
            driver = webdriver.Chrome(service=service, options=options)
            print("Chrome ë“œë¼ì´ë²„ ìƒì„± ì„±ê³µ!")

            # WebDriver ì†ì„± ìˆ¨ê¸°ê¸°
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            driver.set_page_load_timeout(60)
            return driver
        except Exception as e:
            print(f"!!!!!!!! Chrome ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨ !!!!!!!!!!")
            print(f"ì˜¤ë¥˜: {e}")
            # WebDriverManager ë¡œê·¸ í™•ì¸ì„ ìœ„í•´ traceback ì¶”ê°€
            import traceback
            traceback.print_exc()
            raise # ì—ëŸ¬ ë‹¤ì‹œ ë°œìƒ
        else:
            print("  (ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰)")
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument(f'--user-agent={user_agent_string}')
            options.add_argument('--window-size=1920,1080')
            driver = webdriver.Chrome(options=options)

        # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ê³µí†µ)
        driver.set_page_load_timeout(60)
        return driver

    def _scrape_page(self, driver, page_num):
        """íŠ¹ì • í˜ì´ì§€ í¬ë¡¤ë§"""

        items = []
        html = None

        try:
            # í˜ì´ì§€ URL
            if page_num == 1:
                url = self.main_url

                try:
                    boardSelector = "//a[@class='text_center special_dot' and contains(., 'í•«ë”œ')]"
                    board_element = WebDriverWait(driver, 30).until(
                        EC.presence_of_element_located((By.XPATH, boardSelector))
                    )
                    
                    # element.click()
                    ActionChains(driver).move_to_element(board_element).click().perform()
                    print("í•«ë”œ ê²Œì‹œíŒ í´ë¦­")
                    time.sleep(random.uniform(1, 3))

                except TimeoutException:
                    print("Timeout: 30ì´ˆ ì•ˆì— 'í•«ë”œ' ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                except NoSuchElementException:
                    print("ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. XPathë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")

                except ElementClickInterceptedException:
                    print("ë‹¤ë¥¸ ìš”ì†Œê°€ í´ë¦­ì„ ê°€ë¡œë§‰ê³  ìˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¡¤ì´ë‚˜ ëŒ€ê¸° ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                except ElementNotInteractableException:
                    print("ìš”ì†Œê°€ í˜„ì¬ í´ë¦­ ê°€ëŠ¥í•œ ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤ (ì˜ˆ: ìˆ¨ê²¨ì ¸ ìˆìŒ).")

                except Exception as e:
                    print(f"í•«ë”œ ê²Œì‹œíŒ í´ë¦­ ì‹¤íŒ¨, ì§ì ‘ ì´ë™: {e}")
                    driver.get(self.url)
                    time.sleep(2)
            else:
                # 2í˜ì´ì§€ ì´ìƒ
                try:
                    # url = f"{self.main_url}/index.php?mid=hotdeal&page={page_num}"
                    nextPagePath = f"//*[@class='bd_pg clear']//a[normalize-space()='{page_num}']"
                    WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.XPATH, nextPagePath))
                    )
                    nextPageEl = driver.find_element(By.XPATH, nextPagePath)
                    ActionChains(driver).move_to_element(nextPageEl).click().perform()
                except Exception as e:
                    print(f"í•«ë”œ ê²Œì‹œíŒ í´ë¦­ ì‹¤íŒ¨, ì§ì ‘ ì´ë™: {e}")
                    url = f"{self.url}&page={page_num}"
                    driver.get(url)
                    time.sleep(2)

            # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    driver.get(url)
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        print(f"  ì¬ì‹œë„ {attempt + 1}/{max_retries}...")
                        time.sleep(10)
                    else:
                        print(f"  ìµœì¢… ì‹¤íŒ¨: {e}")
                        raise

            try:
                WebDriverWait(driver, 30).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, ".board_list_table"))
                )
                print("ê²Œì‹œê¸€ ë¡œë“œ í™•ì¸")
            except Exception as e:
                # íƒ€ì„ì•„ì›ƒ ë˜ì–´ë„ ê³„ì† ì§„í–‰ (ë¶€ë¶„ ë°ì´í„°ë¼ë„ ìˆ˜ì§‘)
                print(f"ëª…ì‹œì  ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ (ê³„ì† ì§„í–‰): {e}")


            # HTML íŒŒì‹±
            html = driver.page_source
            soup = BeautifulSoup(html, 'lxml')

            # ê²Œì‹œê¸€ ëª©ë¡
            rows = soup.select('.board_list_table tbody tr.blocktarget')
            print(f"í˜ì´ì§€ {page_num}: {len(rows)}ê°œ ë°œê²¬")

            # ë‹¤ë¥¸ ì„ íƒìë“¤ë„ ì‹œë„
            if len(rows) == 0:
                print(f"  [DEBUG] ë‹¤ë¥¸ ì„ íƒì ì‹œë„...")
                alternative_rows = soup.select('.board_list_table')
                print(f"  [DEBUG] ëŒ€ì²´ ì„ íƒì: {len(alternative_rows)}ê°œ")

            if not rows:
                # ê²Œì‹œê¸€ì´ 0ê°œì¼ ë•Œë„ ë””ë²„ê¹… íŒŒì¼ ì €ì¥
                print(f"ê²Œì‹œê¸€ì´ 0ê°œì…ë‹ˆë‹¤. í˜„ì¬ í˜ì´ì§€ ìƒíƒœë¥¼ ë””ë²„ê¹…ìš©ìœ¼ë¡œ S3ì— ì €ì¥í•©ë‹ˆë‹¤.")
                self._save_debug_files_to_s3(driver, error_prefix="no_posts_found")

            for row in rows:
                try:
                    # í•„í„°ë§ : ê³µì§€ / ê´‘ê³  ì œì™¸
                    classes = row.get('class', [])
                    if 'best' in classes:
                        continue

                    # ë°ì´í„° ì¶”ì¶œ
                    item = self._extract_item(row)
                    if item:
                        items.append(item)

                except Exception as e:
                    print(f"ê²Œì‹œê¸€ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue


        except Exception as e:
            # ì´ ê²½ìš°ì—ë„ í˜„ì¬ ìƒíƒœ ì €ì¥
            self._save_debug_files_to_s3(driver, error_prefix="page_load_error")
            import traceback
            traceback.print_exc()

        finally:
            html = None
            soup = None

        return items

    def _extract_item(self, row):
        """
        ì„¸ì¼ì •ë³´ ì¶”ì¶œ
        """
        # ì œëª©
        title_element = row.select_one('a.subject_link.deco')
        raw_title = title_element.get_text(strip=True) if title_element else None
        title = clean_title(raw_title)
        # reply_count = extract_comment_count_from_title(title)


        # íŒ¨í„´ 1: ì œëª© ë§¨ ë (n / m) â†’ n=ê°€ê²©, m=ë°°ì†¡ë¹„
        price = None
        shipping_fee = None
        price_shipping_pattern = r'\(([0-9,\.]+)\s*/\s*(.+?)\)\s*$'
        match = re.search(price_shipping_pattern, raw_title)
        if match:
            price_text = match.group(1).replace(',', '')
            shipping_text = match.group(2).strip()

            try:
                # ê°€ê²©
                price = float(price_text)
            except:
                pass

            # ë°°ì†¡ë¹„
            shipping_fee = extract_shipping_fee(shipping_text)

            # ê°€ê²©/ë°°ì†¡ë¹„ ì œê±°í•œ ì œëª©
            title = title[:match.start()].strip()

        # íŒ¨í„´ 2: ì¼ë°˜ ê°€ê²© ì¶”ì¶œ (common util)
        if price is None:
            price = extract_price_from_text(title)

        # íŒë§¤ì²˜
        # 1) element í™•ì¸
        # 2) ì œëª©ì—ì„œ [íŒë§¤ì²˜] ì¶”ì¶œ
        store_element = row.select_one('span.subject_tag')
        if store_element:
            store = store_element.get_text(strip=True)
            store = clean_store_name(store)
        else:
            store = 'ê¸°íƒ€'

        # ëŒ“ê¸€ ìˆ˜
        reply_element = row.select_one('span.num_reply')
        reply_count = reply_element.get_text(strip=True) if reply_element else 0
        reply_count = extract_number_from_text(reply_count)

        # ì¢‹ì•„ìš” ìˆ˜
        like_element = row.select_one('td.recomd')
        like_count = like_element.get_text(strip=True) if like_element else 0

        # ë“±ë¡ ì‹œê°„
        time_element = row.select_one('td.time')
        if time_element:
            time_text = time_element.get_text(strip=True)
            # print(f"  [DEBUG] ì›ë³¸ ì‹œê°„: {time_text}")
            time_obj = parse_time(time_text)
            if not time_obj:
                print(f"  [DEBUG] íŒŒì‹± ì‹¤íŒ¨!")
            time = to_iso8601(time_obj) if time_obj else None

        # ì¹´í…Œê³ ë¦¬
        category_element = row.select_one('td.divsn.text_over a')
        category = category_element.get_text(strip=True) if category_element else None

        # URL
        product_url = title_element['href']

        # ì´ë¯¸ì§€ url
        image_url = None


        return {
            'title': title,
            'price': price,
            'storeName': store,
            'category': category,
            'shippingFee': shipping_fee,
            'productUrl': product_url,
            'imageUrl': image_url,
            'replyCount': reply_count,
            'likeCount': like_count,
            'sourceSite': self.source_site,
            'crawledAt': time
        }

    def _save_debug_files_to_s3(self, driver, error_prefix):
        """
        ì—ëŸ¬ ë°œìƒ ì‹œ ìŠ¤í¬ë¦°ìƒ·ê³¼ HTMLì„ S3ì— ì €ì¥

        Args:
            driver: Selenium WebDriver ì¸ìŠ¤í„´ìŠ¤
            error_prefix: íŒŒì¼ëª… ì ‘ë‘ì‚¬ (ì˜ˆ: "no_posts_found", "page_load_error")

        ëª©ì :
            - í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë””ë²„ê¹… ìš©ì´ì„± í™•ë³´
            - í¬ë¡¤ë§ ì‹¤íŒ¨ ì›ì¸ ì¶”ì 

        íš¨ê³¼:
            - CloudWatch ë¡œê·¸ë§Œìœ¼ë¡œ ë¶€ì¡±í•œ ì •ë³´ ë³´ì™„
            - ì‹¤ì œ í˜ì´ì§€ ìƒíƒœ ì‹œê°ì  í™•ì¸ ê°€ëŠ¥
        """
        if not self.s3_bucket_name:
            print("âš ï¸ S3 ë²„í‚· ì´ë¦„ì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ë””ë²„ê·¸ íŒŒì¼ì„ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        try:
            kst = datetime.timezone(datetime.timedelta(hours=9))
            timestamp = datetime.datetime.now(kst).strftime('%Y%m%d_%H%M%S')
            s3_client = boto3.client('s3')

            # 1. HTML ì†ŒìŠ¤ ì €ì¥
            html_filename = f"{error_prefix}_{timestamp}.html"
            html_content = driver.page_source
            s3_client.put_object(
                Bucket=self.s3_bucket_name,
                Key=f"debug/{self.source_site}/{html_filename}",
                Body=html_content.encode('utf-8'),
                ContentType='text/html'
            )
            print(f"âœ… HTML ì €ì¥: s3://{self.s3_bucket_name}/debug/{self.source_site}/{html_filename}")

            # 2. ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            screenshot_filename = f"{error_prefix}_{timestamp}.png"
            screenshot_data = driver.get_screenshot_as_png()
            s3_client.put_object(
                Bucket=self.s3_bucket_name,
                Key=f"debug/{self.source_site}/{screenshot_filename}",
                Body=screenshot_data,
                ContentType='image/png'
            )
            print(f"âœ… ìŠ¤í¬ë¦°ìƒ· ì €ì¥: s3://{self.s3_bucket_name}/debug/{self.source_site}/{screenshot_filename}")

        except Exception as e:
            print(f"âŒ S3 ë””ë²„ê·¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
