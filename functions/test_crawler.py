"""
ì‚¬ì´íŠ¸ë³„ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ê° ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import json
from datetime import datetime

# í…ŒìŠ¤íŠ¸í•  ì‚¬ì´íŠ¸ ì„ íƒ
SITES = {
    'ppomppu': {
        'path': 'functions/ppomppu',
        'class': 'PpomppuScraper'
    },
    'ruliweb': {
        'path': 'functions/ruliweb',
        'class': 'RuliwebScraper'
    },
    'fmkorea': {
        'path': 'functions/fmkorea',
        'class': 'FmkoreaScraper'
    },
    'quasarzone': {
        'path': 'functions/quasarzone',
        'class': 'QuasarzoneScraper'
    },
    'arcalive': {
        'path': 'functions/arcalive',
        'class': 'ArcaliveScraper'
    },
    'eomisae': {
        'path': 'functions/eomisae',
        'class': 'EomisaeScraper'
    }
}


def test_site(site_name):
    """
    íŠ¹ì • ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
    """
    if site_name not in SITES:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site_name}")
        print(f"ì‚¬ìš© ê°€ëŠ¥: {', '.join(SITES.keys())}")
        return

    site_info = SITES[site_name]

    print("=" * 60)
    print(f"ğŸ” {site_name.upper()} í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    try:
        # ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), site_info['path']))

        # ìŠ¤í¬ë˜í¼ import
        scraper_module = __import__('scraper')
        scraper_class = getattr(scraper_module, site_info['class'])

        # ìŠ¤í¬ë˜í¼ ì‹¤í–‰
        scraper = scraper_class()
        scraper.test_mode = True  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”

        print("âœ… í¬ë¡¤ë§ ì‹œì‘...")
        items = scraper.scrape()

        print()
        print("=" * 60)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼")
        print("=" * 60)
        print(f"ìˆ˜ì§‘ ê°œìˆ˜: {len(items)}ê°œ")
        print()

        if items:
            # ì²˜ìŒ 3ê°œ í•­ëª© ì¶œë ¥
            for i, item in enumerate(items[:3], 1):
                print(f"[{i}] {item.get('title', 'N/A')}")
                print(f"    ê°€ê²©: {item.get('price_str', item.get('price', 'N/A'))}")
                print(f"    íŒë§¤ì²˜: {item.get('storeName', 'N/A')}")
                print(f"    URL: {item.get('productUrl', 'N/A')[:60]}...")
                print(f"    ì‹œê°„: {item.get('crawledAt', 'N/A')}")
                print()

            if len(items) > 3:
                print(f"... ì™¸ {len(items) - 3}ê°œ")
                print()

            # JSON íŒŒì¼ë¡œ ì €ì¥
            output_file = f"test_output_{site_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(items, f, ensure_ascii=False, indent=2)

            print(f"âœ… ê²°ê³¼ ì €ì¥: {output_file}")
        else:
            print("âš ï¸  í¬ë¡¤ë§ëœ í•­ëª© ì—†ìŒ (30ë¶„ ì´ë‚´ ê²Œì‹œê¸€ ì—†ê±°ë‚˜ ì—ëŸ¬)")

        print()
        print("=" * 60)
        print(f"ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)

    except Exception as e:
        print()
        print("=" * 60)
        print("âŒ ì—ëŸ¬ ë°œìƒ")
        print("=" * 60)
        print(f"{type(e).__name__}: {str(e)}")

        import traceback
        print()
        print("ìƒì„¸ ì—ëŸ¬:")
        traceback.print_exc()


def test_all_sites():
    """
    ëª¨ë“  ì‚¬ì´íŠ¸ ìˆœì°¨ í…ŒìŠ¤íŠ¸
    """
    print()
    print("=" * 60)
    print("ğŸš€ ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print()

    results = {}

    for site_name in SITES.keys():
        try:
            test_site(site_name)
            results[site_name] = "âœ… ì„±ê³µ"
        except Exception as e:
            results[site_name] = f"âŒ ì‹¤íŒ¨: {str(e)}"

        print()
        print()

    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("=" * 60)
    print("ğŸ“‹ ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    for site, result in results.items():
        print(f"{site:15s}: {result}")
    print("=" * 60)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸')
    parser.add_argument(
        'site',
        nargs='?',
        choices=list(SITES.keys()) + ['all'],
        help='í…ŒìŠ¤íŠ¸í•  ì‚¬ì´íŠ¸ (all: ì „ì²´)'
    )

    args = parser.parse_args()

    if not args.site:
        print("ì‚¬ìš©ë²•:")
        print(f"  python test_crawler.py [ì‚¬ì´íŠ¸ëª…]")
        print()
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸:")
        for site in SITES.keys():
            print(f"  - {site}")
        print(f"  - all (ì „ì²´ í…ŒìŠ¤íŠ¸)")
        sys.exit(1)

    if args.site == 'all':
        test_all_sites()
    else:
        test_site(args.site)