"""
ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python test_crawlers.py                    # ëª¨ë“  ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
    python test_crawlers.py ppomppu            # íŠ¹ì • ì‚¬ì´íŠ¸ë§Œ í…ŒìŠ¤íŠ¸
    python test_crawlers.py ppomppu ruliweb    # ì—¬ëŸ¬ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸

ì§€ì› ì‚¬ì´íŠ¸: ppomppu, ruliweb, fmkorea, quasarzone, arcalive, eomisae
"""

import sys
import os
import json
import importlib.util
from datetime import datetime
import argparse

# ì‚¬ì´íŠ¸ë³„ ì„¤ì •
SITES_CONFIG = {
    'ppomppu': {
        'name': 'ë½ë¿Œ',
        'path': 'ppomppu/scraper.py',
        'class_name': 'PpomppuScraper'
    },
    'ruliweb': {
        'name': 'ë£¨ë¦¬ì›¹',
        'path': 'ruliweb/scraper.py',
        'class_name': 'RuliwebScraper'
    },
    'fmkorea': {
        'name': 'FMì½”ë¦¬ì•„',
        'path': 'fmkorea/scraper.py',
        'class_name': 'FmkoreaScraper'
    },
    'quasarzone': {
        'name': 'í€˜ì´ì‚¬ì¡´',
        'path': 'quasarzone/scraper.py',
        'class_name': 'QuasarzoneScraper'
    },
    'arcalive': {
        'name': 'ì•„ì¹´ë¼ì´ë¸Œ',
        'path': 'arcalive/scraper.py',
        'class_name': 'ArcaliveScraper'
    },
    'eomisae': {
        'name': 'ì–´ë¯¸ìƒˆ',
        'path': 'eomisae/scraper.py',
        'class_name': 'EomisaeScraper'
    }
}


def load_scraper_class(site_key):
    """
    ë™ì ìœ¼ë¡œ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ë¥¼ ë¡œë“œ
    
    Args:
        site_key: ì‚¬ì´íŠ¸ í‚¤ (ì˜ˆ: 'ppomppu')
    
    Returns:
        Scraper í´ë˜ìŠ¤
    """
    if site_key not in SITES_CONFIG:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site_key}")
    
    config = SITES_CONFIG[site_key]
    script_dir = os.path.dirname(os.path.abspath(__file__))
    scraper_path = os.path.join(script_dir, config['path'])
    
    if not os.path.exists(scraper_path):
        raise FileNotFoundError(f"ìŠ¤í¬ë˜í¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scraper_path}")
    
    # ëª¨ë“ˆ ë™ì  ë¡œë“œ
    spec = importlib.util.spec_from_file_location(f"{site_key}_scraper", scraper_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    
    # í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    scraper_class = getattr(module, config['class_name'])
    return scraper_class


def test_single_site(site_key, save_json=True, filter_minutes=120):
    """
    ê°œë³„ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
    
    Args:
        site_key: ì‚¬ì´íŠ¸ í‚¤
        save_json: JSON íŒŒì¼ ì €ì¥ ì—¬ë¶€
        filter_minutes: í•„í„°ë§ ì‹œê°„ (ê¸°ë³¸ 120ë¶„)
    
    Returns:
        dict: í…ŒìŠ¤íŠ¸ ê²°ê³¼
    """
    config = SITES_CONFIG[site_key]
    site_name = config['name']
    
    print(f"\n{'='*70}")
    print(f"ğŸ” [{site_name}] í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"{'='*70}")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"í•„í„°ë§ ì‹œê°„: {filter_minutes}ë¶„")
    print()
    
    result = {
        'site_key': site_key,
        'site_name': site_name,
        'success': False,
        'count': 0,
        'duration': 0,
        'error': None,
        'sample_items': []
    }
    
    try:
        # ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ ë¡œë“œ
        print(f"ğŸ“¦ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤ ë¡œë“œ: {config['class_name']}")
        scraper_class = load_scraper_class(site_key)
        
        # ìŠ¤í¬ë˜í¼ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        scraper = scraper_class()
        
        # í•„í„°ë§ ì‹œê°„ ì„¤ì •
        if hasattr(scraper, 'filter_minutes'):
            scraper.filter_minutes = filter_minutes
            print(f"â° í•„í„°ë§ ì‹œê°„ ì„¤ì •: {filter_minutes}ë¶„")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘...\n")
        start_time = datetime.now()
        
        items = scraper.scrape()
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # ê²°ê³¼ ì €ì¥
        result['success'] = True
        result['count'] = len(items)
        result['duration'] = round(duration, 2)
        result['sample_items'] = items[:3] if items else []
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*70}")
        print(f"âœ… [{site_name}] í¬ë¡¤ë§ ì™„ë£Œ")
        print(f"{'='*70}")
        print(f"ìˆ˜ì§‘ ê°œìˆ˜: {len(items)}ê°œ")
        print(f"ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
        print()
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        if items:
            print(f"ğŸ“Š ìƒ˜í”Œ ë°ì´í„° (ìµœëŒ€ 3ê°œ):")
            for i, item in enumerate(items[:3], 1):
                print(f"\n[{i}] {item.get('title', 'N/A')[:60]}...")
                print(f"    ê°€ê²©: {item.get('price', 'N/A')}")
                print(f"    íŒë§¤ì²˜: {item.get('storeName', 'N/A')}")
                print(f"    ì‹œê°„: {item.get('crawledAt', 'N/A')}")
                
                # URL ê¸¸ì´ ì²´í¬
                product_url = item.get('productUrl', '')
                if product_url:
                    print(f"    URL: {product_url[:60]}...")
            
            if len(items) > 3:
                print(f"\n... ì™¸ {len(items) - 3}ê°œ")
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ['title', 'productUrl', 'sourceSite', 'crawledAt']
            sample = items[0]
            missing_fields = [f for f in required_fields if f not in sample or not sample[f]]
            
            if missing_fields:
                print(f"\nâš ï¸  ê²½ê³ : ëˆ„ë½ëœ í•„ìˆ˜ í•„ë“œ - {missing_fields}")
            else:
                print(f"\nâœ… í•„ìˆ˜ í•„ë“œ ê²€ì¦ í†µê³¼")
            
            # JSON ì €ì¥
            if save_json:
                output_file = f"test_result_{site_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(items, f, ensure_ascii=False, indent=2)
                print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
        else:
            print("âš ï¸  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("   - í•„í„°ë§ ì‹œê°„ì„ ëŠ˜ë ¤ë³´ì„¸ìš” (--minutes ì˜µì…˜)")
            print("   - í•´ë‹¹ ì‹œê°„ëŒ€ì— ê²Œì‹œê¸€ì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        return result
        
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        result['error'] = "ì‚¬ìš©ì ì¤‘ë‹¨"
        return result
        
    except Exception as e:
        print(f"\n{'='*70}")
        print(f"âŒ [{site_name}] í¬ë¡¤ë§ ì‹¤íŒ¨")
        print(f"{'='*70}")
        print(f"ì—ëŸ¬: {str(e)}")
        print()
        
        # ìƒì„¸ ì—ëŸ¬ ì¶œë ¥
        import traceback
        print("ìƒì„¸ ì—ëŸ¬:")
        traceback.print_exc()
        
        result['error'] = str(e)
        return result


def test_multiple_sites(site_keys, save_json=True, filter_minutes=60):
    """
    ì—¬ëŸ¬ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
    
    Args:
        site_keys: í…ŒìŠ¤íŠ¸í•  ì‚¬ì´íŠ¸ í‚¤ ë¦¬ìŠ¤íŠ¸
        save_json: JSON íŒŒì¼ ì €ì¥ ì—¬ë¶€
        filter_minutes: í•„í„°ë§ ì‹œê°„
    
    Returns:
        list: ê° ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼
    """
    print("\n" + "="*70)
    print("ğŸš€ ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ í†µí•© í…ŒìŠ¤íŠ¸")
    print("="*70)
    print(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {', '.join([SITES_CONFIG[k]['name'] for k in site_keys])}")
    print(f"ì´ {len(site_keys)}ê°œ ì‚¬ì´íŠ¸")
    print()
    
    results = []
    
    for i, site_key in enumerate(site_keys, 1):
        print(f"\n[{i}/{len(site_keys)}] {SITES_CONFIG[site_key]['name']} í…ŒìŠ¤íŠ¸ ì¤‘...")
        result = test_single_site(site_key, save_json, filter_minutes)
        results.append(result)
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print("\n\n" + "="*70)
    print("ğŸ“Š ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*70)
    
    success_count = sum(1 for r in results if r['success'])
    total_items = sum(r['count'] for r in results)
    total_duration = sum(r['duration'] for r in results)
    
    print(f"\nì„±ê³µ: {success_count}/{len(site_keys)} ì‚¬ì´íŠ¸")
    print(f"ì´ ìˆ˜ì§‘: {total_items}ê°œ")
    print(f"ì´ ì†Œìš”ì‹œê°„: {total_duration:.2f}ì´ˆ")
    print()
    
    # ì‚¬ì´íŠ¸ë³„ ê²°ê³¼
    print("ì‚¬ì´íŠ¸ë³„ ìƒì„¸ ê²°ê³¼:")
    print("-" * 70)
    for r in results:
        status = "âœ…" if r['success'] else "âŒ"
        print(f"{status} {r['site_name']:10s} | {r['count']:4d}ê°œ | {r['duration']:6.2f}ì´ˆ", end="")
        if r['error']:
            print(f" | ì—ëŸ¬: {r['error'][:30]}...")
        else:
            print()
    
    print()
    
    # ì‹¤íŒ¨í•œ ì‚¬ì´íŠ¸ ì•ˆë‚´
    failed_sites = [r for r in results if not r['success']]
    if failed_sites:
        print("âš ï¸  ì‹¤íŒ¨í•œ ì‚¬ì´íŠ¸:")
        for r in failed_sites:
            print(f"   - {r['site_name']}: {r['error']}")
        print()
    
    return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  python test_crawlers.py                      # ëª¨ë“  ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
  python test_crawlers.py ppomppu              # ë½ë¿Œë§Œ í…ŒìŠ¤íŠ¸
  python test_crawlers.py ppomppu ruliweb      # ë½ë¿Œ, ë£¨ë¦¬ì›¹ í…ŒìŠ¤íŠ¸
  python test_crawlers.py --minutes 60         # 60ë¶„ í•„í„°ë§ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
  python test_crawlers.py --no-save            # JSON ì €ì¥ ì•ˆí•¨
  python test_crawlers.py --list               # ì§€ì› ì‚¬ì´íŠ¸ ëª©ë¡

ì§€ì› ì‚¬ì´íŠ¸:
  ppomppu, ruliweb, fmkorea, quasarzone, arcalive, eomisae
        """
    )
    
    parser.add_argument(
        'sites',
        nargs='*',
        help='í…ŒìŠ¤íŠ¸í•  ì‚¬ì´íŠ¸ (ì—†ìœ¼ë©´ ì „ì²´)'
    )
    
    parser.add_argument(
        '--minutes',
        type=int,
        default=120,
        help='í•„í„°ë§ ì‹œê°„ (ê¸°ë³¸: 120ë¶„)'
    )
    
    parser.add_argument(
        '--no-save',
        action='store_true',
        help='JSON íŒŒì¼ ì €ì¥ ì•ˆí•¨'
    )
    
    parser.add_argument(
        '--list',
        action='store_true',
        help='ì§€ì› ì‚¬ì´íŠ¸ ëª©ë¡ ì¶œë ¥'
    )
    
    args = parser.parse_args()
    
    # ì‚¬ì´íŠ¸ ëª©ë¡ ì¶œë ¥
    if args.list:
        print("\nì§€ì›í•˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡:")
        print("-" * 40)
        for key, config in SITES_CONFIG.items():
            print(f"  {key:12s} - {config['name']}")
        print()
        return
    
    # í…ŒìŠ¤íŠ¸í•  ì‚¬ì´íŠ¸ ê²°ì •
    if args.sites:
        # ëª…ë ¹í–‰ ì¸ìë¡œ ì§€ì •ëœ ì‚¬ì´íŠ¸
        invalid_sites = [s for s in args.sites if s not in SITES_CONFIG]
        if invalid_sites:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {', '.join(invalid_sites)}")
            print(f"ì‚¬ìš© ê°€ëŠ¥: {', '.join(SITES_CONFIG.keys())}")
            return
        test_sites = args.sites
    else:
        # ì „ì²´ ì‚¬ì´íŠ¸
        test_sites = list(SITES_CONFIG.keys())
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    save_json = not args.no_save
    
    if len(test_sites) == 1:
        # ë‹¨ì¼ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
        test_single_site(test_sites[0], save_json, args.minutes)
    else:
        # ë‹¤ì¤‘ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
        test_multiple_sites(test_sites, save_json, args.minutes)


if __name__ == "__main__":
    main()
