#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í¬ë¡¤ëŸ¬ ê¸´ê¸‰ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë£¨ë¦¬ì›¹, ë½ë¿Œ, ì—í¨ì½”ë¦¬ì•„ ì‘ë™ ì—¬ë¶€ í™•ì¸
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

# í…ŒìŠ¤íŠ¸í•  í¬ë¡¤ëŸ¬ ì„¤ì •
CRAWLERS = {
    'PPOMPPU': {
        'url': 'https://www.ppomppu.co.kr/zboard/zboard.php?id=ppomppu',
        'selector': 'tr[class*="list"]'
    },
    'RULIWEB': {
        'url': 'https://bbs.ruliweb.com/market/board/1020',
        'selector': '.table_body tr'
    },
    'FMKOREA': {
        'url': 'https://www.fmkorea.com/hotdeal',
        'selector': '.li_best_wrapper li'
    }
}

def test_connection(site_name, url):
    """ì‚¬ì´íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print(f"\n{'='*50}")
    print(f"ğŸ§ª í…ŒìŠ¤íŠ¸: {site_name}")
    print(f"{'='*50}")
    print(f"ğŸ“ URL: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            print(f"âœ… ì—°ê²° ì„±ê³µ: {response.status_code}")
            return response.text
        else:
            print(f"âŒ ì—°ê²° ì‹¤íŒ¨: {response.status_code}")
            return None
            
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {str(e)}")
        return None

def test_parsing(html_content, selector):
    """HTML íŒŒì‹± í…ŒìŠ¤íŠ¸"""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        items = soup.select(selector)
        
        if len(items) > 0:
            print(f"âœ… íŒŒì‹± ì„±ê³µ: {len(items)}ê°œ ì•„ì´í…œ ë°œê²¬")
            return True
        else:
            print(f"âŒ íŒŒì‹± ì‹¤íŒ¨: ì•„ì´í…œ ì—†ìŒ")
            print(f"   ì‹œë„í•œ ì„ íƒì: {selector}")
            return False
            
    except Exception as e:
        print(f"âŒ íŒŒì‹± ì—ëŸ¬: {str(e)}")
        return False

def main():
    print("ğŸš€ 3ê°œ í¬ë¡¤ëŸ¬ ê¸´ê¸‰ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    results = {}
    
    for site_name, config in CRAWLERS.items():
        html = test_connection(site_name, config['url'])
        
        if html:
            success = test_parsing(html, config['selector'])
            results[site_name] = success
        else:
            results[site_name] = False
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*50}")
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print(f"{'='*50}")
    
    success_count = sum(results.values())
    total_count = len(results)
    
    for site, success in results.items():
        status = "âœ… ì •ìƒ" if success else "âŒ ë¬¸ì œ"
        print(f"{site}: {status}")
    
    print(f"\nì„±ê³µë¥ : {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
    
    if success_count == 0:
        print("\nâš ï¸ ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤íŒ¨!")
        print("ê°€ëŠ¥í•œ ì›ì¸:")
        print("1. ì‚¬ì´íŠ¸ ì ‘ì† ì°¨ë‹¨ (IP/Bot ì°¨ë‹¨)")
        print("2. HTML êµ¬ì¡° ë³€ê²½")
        print("3. ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ")
        print("\nğŸ’¡ í•´ê²°ì±…:")
        print("- requests_html ë˜ëŠ” selenium ì‚¬ìš©")
        print("- User-Agent ë³€ê²½")
        print("- Proxy ì‚¬ìš©")
    elif success_count < total_count:
        failed = [k for k, v in results.items() if not v]
        print(f"\nâš ï¸ ì¼ë¶€ í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: {', '.join(failed)}")
    else:
        print("\nğŸ‰ ëª¨ë“  í¬ë¡¤ëŸ¬ ì •ìƒ ì‘ë™!")

if __name__ == "__main__":
    main()
